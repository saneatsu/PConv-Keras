{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "h5file = '/mnt/PConv-Keras/data/model/weight-crop-512-1024/1_weights_2018-10-27-05-22-52.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert element 0 (beta:0) to hsize_t",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mh5py/utils.pyx\u001b[0m in \u001b[0;36mh5py.utils.convert_tuple\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3c200cd41a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         \u001b[0mwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pconv-Keras/lib/python3.6/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pconv-Keras/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNULL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5s.pyx\u001b[0m in \u001b[0;36mh5py.h5s.create_simple\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/utils.pyx\u001b[0m in \u001b[0;36mh5py.utils.convert_tuple\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert element 0 (beta:0) to hsize_t"
     ]
    }
   ],
   "source": [
    "rfpath = '/mnt/PConv-Keras/data/model/weight-crop-512-1024/1_weights_2018-10-27-05-22-52.h5'\n",
    "wfpath = \"/mnt/PConv-Keras/data/model/weight-crop-512-1024/1_weights_2018-10-27-05-22-52_new.h5\"\n",
    "with h5py.File(rfpath, \"r\") as rf:\t\n",
    "\twith h5py.File(wfpath, \"w\") as wf:\n",
    "\t\tfor k in rf[\"model_2\"].keys():\n",
    "\t\t\twf.create_dataset(k, rf[\"model_2\"][k])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)----- 1st level -----\n",
      "conv2d_1\n",
      "input_3\n",
      "input_4\n",
      "lambda_1\n",
      "lambda_2\n",
      "lambda_3\n",
      "lambda_4\n",
      "lambda_5\n",
      "lambda_6\n",
      "lambda_7\n",
      "lambda_8\n",
      "model_2\n",
      "(2)----- 2nd level, Group \"entry1\" -----\n",
      "EncBN1\n",
      "EncBN2\n",
      "EncBN3\n",
      "EncBN4\n",
      "EncBN5\n",
      "EncBN6\n",
      "EncBN7\n",
      "batch_normalization_1\n",
      "batch_normalization_2\n",
      "batch_normalization_3\n",
      "batch_normalization_4\n",
      "batch_normalization_5\n",
      "batch_normalization_6\n",
      "batch_normalization_7\n",
      "conv2d_1\n",
      "p_conv2d_1\n",
      "p_conv2d_10\n",
      "p_conv2d_11\n",
      "p_conv2d_12\n",
      "p_conv2d_13\n",
      "p_conv2d_14\n",
      "p_conv2d_15\n",
      "p_conv2d_16\n",
      "p_conv2d_2\n",
      "p_conv2d_3\n",
      "p_conv2d_4\n",
      "p_conv2d_5\n",
      "p_conv2d_6\n",
      "p_conv2d_7\n",
      "p_conv2d_8\n",
      "p_conv2d_9\n",
      "(4)----- 2nd level, Group \"model_2\", values() -----\n",
      "<HDF5 group \"/model_2/EncBN1\" (4 members)>\n",
      "<HDF5 group \"/model_2/EncBN2\" (4 members)>\n",
      "<HDF5 group \"/model_2/EncBN3\" (4 members)>\n",
      "<HDF5 group \"/model_2/EncBN4\" (4 members)>\n",
      "<HDF5 group \"/model_2/EncBN5\" (4 members)>\n",
      "<HDF5 group \"/model_2/EncBN6\" (4 members)>\n",
      "<HDF5 group \"/model_2/EncBN7\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_1\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_2\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_3\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_4\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_5\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_6\" (4 members)>\n",
      "<HDF5 group \"/model_2/batch_normalization_7\" (4 members)>\n",
      "<HDF5 group \"/model_2/conv2d_1\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_1\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_10\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_11\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_12\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_13\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_14\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_15\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_16\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_2\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_3\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_4\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_5\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_6\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_7\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_8\" (2 members)>\n",
      "<HDF5 group \"/model_2/p_conv2d_9\" (2 members)>\n",
      "(5)----- 2nd level, Group \"model_2\", items() -----\n",
      "('EncBN1', <HDF5 group \"/model_2/EncBN1\" (4 members)>)\n",
      "('EncBN2', <HDF5 group \"/model_2/EncBN2\" (4 members)>)\n",
      "('EncBN3', <HDF5 group \"/model_2/EncBN3\" (4 members)>)\n",
      "('EncBN4', <HDF5 group \"/model_2/EncBN4\" (4 members)>)\n",
      "('EncBN5', <HDF5 group \"/model_2/EncBN5\" (4 members)>)\n",
      "('EncBN6', <HDF5 group \"/model_2/EncBN6\" (4 members)>)\n",
      "('EncBN7', <HDF5 group \"/model_2/EncBN7\" (4 members)>)\n",
      "('batch_normalization_1', <HDF5 group \"/model_2/batch_normalization_1\" (4 members)>)\n",
      "('batch_normalization_2', <HDF5 group \"/model_2/batch_normalization_2\" (4 members)>)\n",
      "('batch_normalization_3', <HDF5 group \"/model_2/batch_normalization_3\" (4 members)>)\n",
      "('batch_normalization_4', <HDF5 group \"/model_2/batch_normalization_4\" (4 members)>)\n",
      "('batch_normalization_5', <HDF5 group \"/model_2/batch_normalization_5\" (4 members)>)\n",
      "('batch_normalization_6', <HDF5 group \"/model_2/batch_normalization_6\" (4 members)>)\n",
      "('batch_normalization_7', <HDF5 group \"/model_2/batch_normalization_7\" (4 members)>)\n",
      "('conv2d_1', <HDF5 group \"/model_2/conv2d_1\" (2 members)>)\n",
      "('p_conv2d_1', <HDF5 group \"/model_2/p_conv2d_1\" (2 members)>)\n",
      "('p_conv2d_10', <HDF5 group \"/model_2/p_conv2d_10\" (2 members)>)\n",
      "('p_conv2d_11', <HDF5 group \"/model_2/p_conv2d_11\" (2 members)>)\n",
      "('p_conv2d_12', <HDF5 group \"/model_2/p_conv2d_12\" (2 members)>)\n",
      "('p_conv2d_13', <HDF5 group \"/model_2/p_conv2d_13\" (2 members)>)\n",
      "('p_conv2d_14', <HDF5 group \"/model_2/p_conv2d_14\" (2 members)>)\n",
      "('p_conv2d_15', <HDF5 group \"/model_2/p_conv2d_15\" (2 members)>)\n",
      "('p_conv2d_16', <HDF5 group \"/model_2/p_conv2d_16\" (2 members)>)\n",
      "('p_conv2d_2', <HDF5 group \"/model_2/p_conv2d_2\" (2 members)>)\n",
      "('p_conv2d_3', <HDF5 group \"/model_2/p_conv2d_3\" (2 members)>)\n",
      "('p_conv2d_4', <HDF5 group \"/model_2/p_conv2d_4\" (2 members)>)\n",
      "('p_conv2d_5', <HDF5 group \"/model_2/p_conv2d_5\" (2 members)>)\n",
      "('p_conv2d_6', <HDF5 group \"/model_2/p_conv2d_6\" (2 members)>)\n",
      "('p_conv2d_7', <HDF5 group \"/model_2/p_conv2d_7\" (2 members)>)\n",
      "('p_conv2d_8', <HDF5 group \"/model_2/p_conv2d_8\" (2 members)>)\n",
      "('p_conv2d_9', <HDF5 group \"/model_2/p_conv2d_9\" (2 members)>)\n",
      "(6)----- 3rd level, Group \"data1\", items() -----\n",
      "('beta:0', <HDF5 dataset \"beta:0\": shape (256,), type \"<f4\">)\n",
      "('gamma:0', <HDF5 dataset \"gamma:0\": shape (256,), type \"<f4\">)\n",
      "('moving_mean:0', <HDF5 dataset \"moving_mean:0\": shape (256,), type \"<f4\">)\n",
      "('moving_variance:0', <HDF5 dataset \"moving_variance:0\": shape (256,), type \"<f4\">)\n",
      "(7)----- 3rd level, Dataset \"comment\" -----\n",
      "beta:0\n",
      "gamma:0\n",
      "moving_mean:0\n",
      "moving_variance:0\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5file,'r') as f:\n",
    "    print('(1)----- 1st level -----')\n",
    "    # ファイルオブジェクトをイテレートするとファイル直下のオブジェクト名を返す\n",
    "    for k in f:\n",
    "        print(k)\n",
    "        \n",
    "    print('(2)----- 2nd level, Group \"entry1\" -----')\n",
    "    # Groupもそのままforループに渡すと直下にいるオブジェクト名を返す\n",
    "    for k in f['model_2']:\n",
    "        print(k)\n",
    "        \n",
    "#     print('(3)----- 2nd level, Group \"entry1\", keys() -----')\n",
    "#     # ディクショナリーのkeyを表示するようにしても(2)と同じ結果になる\n",
    "#     for k in f['model_2'].keys():\n",
    "#         print(k)        \n",
    "              \n",
    "    print('(4)----- 2nd level, Group \"model_2\", values() -----')\n",
    "    # model_2に入っている実体を表示\n",
    "    for k in f['model_2'].values():\n",
    "        print(k)\n",
    "      \n",
    "    print('(5)----- 2nd level, Group \"model_2\", items() -----')\n",
    "    # model_2に入っているオブジェクト名と実体のtupleを表示\n",
    "    for k in f['model_2'].items():\n",
    "        print(k)        \n",
    "\n",
    "    print('(6)----- 3rd level, Group \"data1\", items() -----')\n",
    "    # entry1/data1というGroupのオブジェクト名と実体のtuple\n",
    "    for k in f['model_2/batch_normalization_1'].items():\n",
    "        print(k)     \n",
    "\n",
    "    print('(7)----- 3rd level, Dataset \"comment\" -----')\n",
    "    # commentはDatasetなので直下にオブジェクトを持たずkeysは使えない\n",
    "    for k in f['model_2/batch_normalization_1'].keys():\n",
    "        print(k)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_1\n",
      "input_3\n",
      "input_4\n",
      "lambda_1\n",
      "lambda_2\n",
      "lambda_3\n",
      "lambda_4\n",
      "lambda_5\n",
      "lambda_6\n",
      "lambda_7\n",
      "lambda_8\n",
      "model_2\n",
      "model_2/EncBN1\n",
      "model_2/EncBN1/beta:0\n",
      "model_2/EncBN1/gamma:0\n",
      "model_2/EncBN1/moving_mean:0\n",
      "model_2/EncBN1/moving_variance:0\n",
      "model_2/EncBN2\n",
      "model_2/EncBN2/beta:0\n",
      "model_2/EncBN2/gamma:0\n",
      "model_2/EncBN2/moving_mean:0\n",
      "model_2/EncBN2/moving_variance:0\n",
      "model_2/EncBN3\n",
      "model_2/EncBN3/beta:0\n",
      "model_2/EncBN3/gamma:0\n",
      "model_2/EncBN3/moving_mean:0\n",
      "model_2/EncBN3/moving_variance:0\n",
      "model_2/EncBN4\n",
      "model_2/EncBN4/beta:0\n",
      "model_2/EncBN4/gamma:0\n",
      "model_2/EncBN4/moving_mean:0\n",
      "model_2/EncBN4/moving_variance:0\n",
      "model_2/EncBN5\n",
      "model_2/EncBN5/beta:0\n",
      "model_2/EncBN5/gamma:0\n",
      "model_2/EncBN5/moving_mean:0\n",
      "model_2/EncBN5/moving_variance:0\n",
      "model_2/EncBN6\n",
      "model_2/EncBN6/beta:0\n",
      "model_2/EncBN6/gamma:0\n",
      "model_2/EncBN6/moving_mean:0\n",
      "model_2/EncBN6/moving_variance:0\n",
      "model_2/EncBN7\n",
      "model_2/EncBN7/beta:0\n",
      "model_2/EncBN7/gamma:0\n",
      "model_2/EncBN7/moving_mean:0\n",
      "model_2/EncBN7/moving_variance:0\n",
      "model_2/batch_normalization_1\n",
      "model_2/batch_normalization_1/beta:0\n",
      "model_2/batch_normalization_1/gamma:0\n",
      "model_2/batch_normalization_1/moving_mean:0\n",
      "model_2/batch_normalization_1/moving_variance:0\n",
      "model_2/batch_normalization_2\n",
      "model_2/batch_normalization_2/beta:0\n",
      "model_2/batch_normalization_2/gamma:0\n",
      "model_2/batch_normalization_2/moving_mean:0\n",
      "model_2/batch_normalization_2/moving_variance:0\n",
      "model_2/batch_normalization_3\n",
      "model_2/batch_normalization_3/beta:0\n",
      "model_2/batch_normalization_3/gamma:0\n",
      "model_2/batch_normalization_3/moving_mean:0\n",
      "model_2/batch_normalization_3/moving_variance:0\n",
      "model_2/batch_normalization_4\n",
      "model_2/batch_normalization_4/beta:0\n",
      "model_2/batch_normalization_4/gamma:0\n",
      "model_2/batch_normalization_4/moving_mean:0\n",
      "model_2/batch_normalization_4/moving_variance:0\n",
      "model_2/batch_normalization_5\n",
      "model_2/batch_normalization_5/beta:0\n",
      "model_2/batch_normalization_5/gamma:0\n",
      "model_2/batch_normalization_5/moving_mean:0\n",
      "model_2/batch_normalization_5/moving_variance:0\n",
      "model_2/batch_normalization_6\n",
      "model_2/batch_normalization_6/beta:0\n",
      "model_2/batch_normalization_6/gamma:0\n",
      "model_2/batch_normalization_6/moving_mean:0\n",
      "model_2/batch_normalization_6/moving_variance:0\n",
      "model_2/batch_normalization_7\n",
      "model_2/batch_normalization_7/beta:0\n",
      "model_2/batch_normalization_7/gamma:0\n",
      "model_2/batch_normalization_7/moving_mean:0\n",
      "model_2/batch_normalization_7/moving_variance:0\n",
      "model_2/conv2d_1\n",
      "model_2/conv2d_1/bias:0\n",
      "model_2/conv2d_1/kernel:0\n",
      "model_2/p_conv2d_1\n",
      "model_2/p_conv2d_1/bias:0\n",
      "model_2/p_conv2d_1/img_kernel:0\n",
      "model_2/p_conv2d_10\n",
      "model_2/p_conv2d_10/bias:0\n",
      "model_2/p_conv2d_10/img_kernel:0\n",
      "model_2/p_conv2d_11\n",
      "model_2/p_conv2d_11/bias:0\n",
      "model_2/p_conv2d_11/img_kernel:0\n",
      "model_2/p_conv2d_12\n",
      "model_2/p_conv2d_12/bias:0\n",
      "model_2/p_conv2d_12/img_kernel:0\n",
      "model_2/p_conv2d_13\n",
      "model_2/p_conv2d_13/bias:0\n",
      "model_2/p_conv2d_13/img_kernel:0\n",
      "model_2/p_conv2d_14\n",
      "model_2/p_conv2d_14/bias:0\n",
      "model_2/p_conv2d_14/img_kernel:0\n",
      "model_2/p_conv2d_15\n",
      "model_2/p_conv2d_15/bias:0\n",
      "model_2/p_conv2d_15/img_kernel:0\n",
      "model_2/p_conv2d_16\n",
      "model_2/p_conv2d_16/bias:0\n",
      "model_2/p_conv2d_16/img_kernel:0\n",
      "model_2/p_conv2d_2\n",
      "model_2/p_conv2d_2/bias:0\n",
      "model_2/p_conv2d_2/img_kernel:0\n",
      "model_2/p_conv2d_3\n",
      "model_2/p_conv2d_3/bias:0\n",
      "model_2/p_conv2d_3/img_kernel:0\n",
      "model_2/p_conv2d_4\n",
      "model_2/p_conv2d_4/bias:0\n",
      "model_2/p_conv2d_4/img_kernel:0\n",
      "model_2/p_conv2d_5\n",
      "model_2/p_conv2d_5/bias:0\n",
      "model_2/p_conv2d_5/img_kernel:0\n",
      "model_2/p_conv2d_6\n",
      "model_2/p_conv2d_6/bias:0\n",
      "model_2/p_conv2d_6/img_kernel:0\n",
      "model_2/p_conv2d_7\n",
      "model_2/p_conv2d_7/bias:0\n",
      "model_2/p_conv2d_7/img_kernel:0\n",
      "model_2/p_conv2d_8\n",
      "model_2/p_conv2d_8/bias:0\n",
      "model_2/p_conv2d_8/img_kernel:0\n",
      "model_2/p_conv2d_9\n",
      "model_2/p_conv2d_9/bias:0\n",
      "model_2/p_conv2d_9/img_kernel:0\n"
     ]
    }
   ],
   "source": [
    "def PrintAllObjects(name):\n",
    "    print(name)\n",
    "    # パスの一覧からも判断できるが\n",
    "    # 実体をprintするとDatasetかGroupかはっきり表示される\n",
    "    # print('\\t', f[name])\n",
    "\n",
    "with h5py.File(h5file,'r') as f:\n",
    "    f.visit(PrintAllObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2/EncBN1/beta:0\n",
      "model_2/EncBN1/gamma:0\n",
      "model_2/EncBN1/moving_mean:0\n",
      "model_2/EncBN1/moving_variance:0\n",
      "model_2/EncBN2/beta:0\n",
      "model_2/EncBN2/gamma:0\n",
      "model_2/EncBN2/moving_mean:0\n",
      "model_2/EncBN2/moving_variance:0\n",
      "model_2/EncBN3/beta:0\n",
      "model_2/EncBN3/gamma:0\n",
      "model_2/EncBN3/moving_mean:0\n",
      "model_2/EncBN3/moving_variance:0\n",
      "model_2/EncBN4/beta:0\n",
      "model_2/EncBN4/gamma:0\n",
      "model_2/EncBN4/moving_mean:0\n",
      "model_2/EncBN4/moving_variance:0\n",
      "model_2/EncBN5/beta:0\n",
      "model_2/EncBN5/gamma:0\n",
      "model_2/EncBN5/moving_mean:0\n",
      "model_2/EncBN5/moving_variance:0\n",
      "model_2/EncBN6/beta:0\n",
      "model_2/EncBN6/gamma:0\n",
      "model_2/EncBN6/moving_mean:0\n",
      "model_2/EncBN6/moving_variance:0\n",
      "model_2/EncBN7/beta:0\n",
      "model_2/EncBN7/gamma:0\n",
      "model_2/EncBN7/moving_mean:0\n",
      "model_2/EncBN7/moving_variance:0\n",
      "model_2/batch_normalization_1/beta:0\n",
      "model_2/batch_normalization_1/gamma:0\n",
      "model_2/batch_normalization_1/moving_mean:0\n",
      "model_2/batch_normalization_1/moving_variance:0\n",
      "model_2/batch_normalization_2/beta:0\n",
      "model_2/batch_normalization_2/gamma:0\n",
      "model_2/batch_normalization_2/moving_mean:0\n",
      "model_2/batch_normalization_2/moving_variance:0\n",
      "model_2/batch_normalization_3/beta:0\n",
      "model_2/batch_normalization_3/gamma:0\n",
      "model_2/batch_normalization_3/moving_mean:0\n",
      "model_2/batch_normalization_3/moving_variance:0\n",
      "model_2/batch_normalization_4/beta:0\n",
      "model_2/batch_normalization_4/gamma:0\n",
      "model_2/batch_normalization_4/moving_mean:0\n",
      "model_2/batch_normalization_4/moving_variance:0\n",
      "model_2/batch_normalization_5/beta:0\n",
      "model_2/batch_normalization_5/gamma:0\n",
      "model_2/batch_normalization_5/moving_mean:0\n",
      "model_2/batch_normalization_5/moving_variance:0\n",
      "model_2/batch_normalization_6/beta:0\n",
      "model_2/batch_normalization_6/gamma:0\n",
      "model_2/batch_normalization_6/moving_mean:0\n",
      "model_2/batch_normalization_6/moving_variance:0\n",
      "model_2/batch_normalization_7/beta:0\n",
      "model_2/batch_normalization_7/gamma:0\n",
      "model_2/batch_normalization_7/moving_mean:0\n",
      "model_2/batch_normalization_7/moving_variance:0\n",
      "model_2/conv2d_1/bias:0\n",
      "model_2/conv2d_1/kernel:0\n",
      "model_2/p_conv2d_1/bias:0\n",
      "model_2/p_conv2d_1/img_kernel:0\n",
      "model_2/p_conv2d_10/bias:0\n",
      "model_2/p_conv2d_10/img_kernel:0\n",
      "model_2/p_conv2d_11/bias:0\n",
      "model_2/p_conv2d_11/img_kernel:0\n",
      "model_2/p_conv2d_12/bias:0\n",
      "model_2/p_conv2d_12/img_kernel:0\n",
      "model_2/p_conv2d_13/bias:0\n",
      "model_2/p_conv2d_13/img_kernel:0\n",
      "model_2/p_conv2d_14/bias:0\n",
      "model_2/p_conv2d_14/img_kernel:0\n",
      "model_2/p_conv2d_15/bias:0\n",
      "model_2/p_conv2d_15/img_kernel:0\n",
      "model_2/p_conv2d_16/bias:0\n",
      "model_2/p_conv2d_16/img_kernel:0\n",
      "model_2/p_conv2d_2/bias:0\n",
      "model_2/p_conv2d_2/img_kernel:0\n",
      "model_2/p_conv2d_3/bias:0\n",
      "model_2/p_conv2d_3/img_kernel:0\n",
      "model_2/p_conv2d_4/bias:0\n",
      "model_2/p_conv2d_4/img_kernel:0\n",
      "model_2/p_conv2d_5/bias:0\n",
      "model_2/p_conv2d_5/img_kernel:0\n",
      "model_2/p_conv2d_6/bias:0\n",
      "model_2/p_conv2d_6/img_kernel:0\n",
      "model_2/p_conv2d_7/bias:0\n",
      "model_2/p_conv2d_7/img_kernel:0\n",
      "model_2/p_conv2d_8/bias:0\n",
      "model_2/p_conv2d_8/img_kernel:0\n",
      "model_2/p_conv2d_9/bias:0\n",
      "model_2/p_conv2d_9/img_kernel:0\n"
     ]
    }
   ],
   "source": [
    "def PrintOnlyDataset(name, obj):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(name)\n",
    "        # print('\\t',obj)\n",
    "\n",
    "with h5py.File(h5file,'r') as f:\n",
    "    f.visititems(PrintOnlyDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeMeasurementList(hdfpaths, datasets, save=False, output='df.h5'):\n",
    "    '''\n",
    "    測定条件をまとめたDataFrameを作成する。\n",
    "\n",
    "    hdfpaths: HDFファイルへのパスのリスト\n",
    "    datasets: HDFファイルから抽出したい情報パスのディクショナリー。\n",
    "              keyはDataFrameのコラム名。\n",
    "    '''\n",
    "\n",
    "    # このheaderというディクショナリーにdatasetsで指定した情報をHDFから読み取り追加\n",
    "    header = {'path':hdfpaths} \n",
    "\n",
    "    for hdfpath in hdfpaths:\n",
    "        with h5py.File(hdfpath,'r') as f:\n",
    "            for colname, dataset in datasets.items():\n",
    "                # 最初のループ\n",
    "                if not colname in header :\n",
    "                    # 最初のループの段階ではheaderには先に定義した'path'しかない\n",
    "                    # 新しいkeyとリストにいれた文字列、数字、行列をvalueとして格納\n",
    "                    if colname == 'total counts':\n",
    "                        header[colname] = [f[dataset].value.sum()]\n",
    "                    elif colname == 'counts':\n",
    "                        header[colname] = [f[dataset].value]\n",
    "                    else:\n",
    "                        # 最初に文字列のデコードにトライ、ダメなら普通の数字としてトライ\n",
    "                        # 値がなければnanを追加\n",
    "                        try:\n",
    "                            header[colname] = [f[dataset].value[0].decode('utf-8')]\n",
    "                        except AttributeError: \n",
    "                            # ここはnumpy arrayをlistに変換\n",
    "                            header[colname] = list(f[dataset].value)\n",
    "                        except KeyError:\n",
    "                            header[colname] = [np.nan]\n",
    "\n",
    "                # 初回以降のループはディクショナリーの該当keyに入ったリストに値を追加していく\n",
    "                else :\n",
    "                    if colname == 'total counts':\n",
    "                        header[colname].append(f[dataset].value.sum())\n",
    "                    elif colname == 'counts':\n",
    "                        header[colname].append(f[dataset].value)\n",
    "                    else:\n",
    "                        try:\n",
    "                            header[colname].append(f[dataset].value[0].decode('utf-8'))\n",
    "                        except AttributeError:\n",
    "                            header[colname].append(f[dataset].value[0])\n",
    "                        except KeyError:\n",
    "                            header[colname].append(np.nan)\n",
    "    # ディクショナリーをDataFrameに変換\n",
    "    df = pd.DataFrame(header)\n",
    "\n",
    "    # saveオプションがTrueならHDFファイルとして保存\n",
    "    if save:\n",
    "        df.to_hdf(output, 'df')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # ファイルパスを便利に扱える標準パッケージ\n",
    "\n",
    "hdfpaths = sorted(Path('./data').glob('**/*.hdf'))\n",
    "# print('No. of HDF files:', len(hdfpaths)) #確認用\n",
    "\n",
    "# HDF5ファイルの抽出したい値の場所を、その値を格納する列の名前をkeyにしたdictionaryに\n",
    "# ここだけは手作業が必要\n",
    "instrument = 'SANS'\n",
    "datasets = {'lambda':f'entry1/{instrument}/Dornier-VS/lambda',\n",
    "            'att':f'entry1/{instrument}/attenuator/selection',\n",
    "            'bsx':f'entry1/{instrument}/beam_stop/x_position',\n",
    "            'bsy':f'entry1/{instrument}/beam_stop/y_position',\n",
    "            'coll':f'entry1/{instrument}/collimator/length',\n",
    "            'bcx':f'entry1/{instrument}/detector/beam_center_x',\n",
    "            'bcy':f'entry1/{instrument}/detector/beam_center_y',\n",
    "            'time':f'entry1/{instrument}/detector/counting_time',\n",
    "            'total counts':f'entry1/{instrument}/detector/counts',\n",
    "            # 'counts':f'entry1/{instrument}/detector/counts',\n",
    "            'SD':f'entry1/{instrument}/detector/x_position',\n",
    "            'mon1':f'entry1/{instrument}/monitor1/counts',\n",
    "            'mon2':f'entry1/{instrument}/monitor2/counts',\n",
    "            'comment':'entry1/comment',\n",
    "            'end_time':'entry1/end_time',\n",
    "            'mf':'entry1/sample/magnetic_field',\n",
    "            'sample_name':'entry1/sample/name',\n",
    "            'sample_z':'entry1/sample/z_position',\n",
    "            'flipper':f'entry1/{instrument}/flipper/state',\n",
    "            'polarizer':f'entry1/{instrument}/polarizer/state',\n",
    "            'start_time':'entry1/start_time'}\n",
    "\n",
    "df = MakeMeasurementList(hdfpaths, datasets)\n",
    "# print(f'No. of scans in df: {len(df)}') #確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PConv-Keras",
   "language": "python",
   "name": "pytorch-segmentation-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
